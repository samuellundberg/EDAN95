{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6, unsupervised learning\n",
    "\n",
    "## Objectives: \n",
    "1. train a Gaussian NBC with the EM algorithm (Expectation - Maximisation)\n",
    "2. compare the results you get to those of the k-Means clustering provided in SciKitLearn\n",
    "3. discuss the classifiers from this lab session and those from the previous session (supervised learning of NBCs) in a brief report\n",
    "\n",
    "## Background and Tools\n",
    "The EM  algorithm solves the problem of not being able to compute the Maximum Likelihood Estimates for unknown classes directly by iterating over two steps until there is no significant change in step 2 observable:\n",
    "\n",
    "1. Compute the expected outcome for each example / sample given estimates for priors and distribution (essentially, the likelihoods for observing the sample assuming an estimated distribution).  \n",
    "2. Compute new estimates for your priors and distributions (in the case of a Gaussian NBC, new means and variances are needed) based on the estimated expected values for how much each sample belongs to the respective distribution.\n",
    "\n",
    "You can find the algorithm stated explicitly as given in Murphy, \"Machine Learning - A probabilistic perspective\", pp 352 - 353 http://fileadmin.cs.lth.se/cs/Education/EDAN95/Handouts/EM-algo.pdf.\n",
    "\n",
    "One special case of the EM algorithm is k-Means clustering, for which an implementation can be found in SciKitLearn.\n",
    "\n",
    "## Your implementation task\n",
    "1. Implement the EM-algorithm to find a Gaussian NBC for the digits dataset from SciKitLearn (you can of course also use the MNIST_Light set from Lab 5, but for initial tests the digits data set is more convenient, since it is smaller in various aspects). You may assume (conditional) independence between the attributes, i.e., the covariances can be assumed to be simply the variances over each attribute. Split the data set in 70% training and 30% test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1257, 64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "digits =  load_digits()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0])\n",
    "plt.show() \n",
    "\n",
    "split = int(len(digits.data)*0.7)\n",
    "\n",
    "data_1_train = digits.data[:split]\n",
    "data_1_test = digits.data[split+1:]\n",
    "labels_1_train = digits.target[:split]\n",
    "labels_1_test = digits.target[split + 1:]\n",
    "np.shape(data_1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.04938606838033364\n",
      "#ERRORS:  0\n",
      "2 0.011173573597267217\n",
      "min pi is:  1.1478959252545075e-05\n",
      "steps at iteration  1 :  1.9903909357162513 1.9842265485074075 \n",
      "\n",
      "1 0.32533522382594116\n",
      "#ERRORS:  0\n",
      "2 0.0024044466388150223\n",
      "min pi is:  6.919811961213282e-08\n",
      "steps at iteration  2 :  1.0723471579451844 1.360785660720532 \n",
      "\n",
      "1 0.32788645252624526\n",
      "#ERRORS:  0\n",
      "2 0.0009462601047949713\n",
      "min pi is:  2.0966086461450085e-08\n",
      "steps at iteration  3 :  0.11320919311703498 0.24102305268122068 \n",
      "\n",
      "1 0.32814821926969295\n",
      "#ERRORS:  0\n",
      "2 0.0009520204730891964\n",
      "min pi is:  1.79280423293713e-08\n",
      "steps at iteration  4 :  0.017547460555408987 0.022742509250965784 \n",
      "\n",
      "1 0.3281483625605881\n",
      "#ERRORS:  0\n",
      "2 0.0009520208115412713\n",
      "min pi is:  1.740082663463787e-08\n",
      "steps at iteration  5 :  0.0027389459256805236 0.003460391583741235 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.00141841e-06, 1.74008266e-08, 3.93924813e-06, 3.95617502e-05,\n",
       "       1.74022264e-06, 9.92325471e-01, 5.27997737e-03, 1.86650990e-03,\n",
       "       1.01619926e-04, 3.75161687e-04])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "X = data_1_train\n",
    "M = np.mean(X)\n",
    "S = np.std(X)\n",
    "X = (X - M) / S\n",
    "K = 10\n",
    "N, n_pix = np.shape(X)\n",
    "# These factors are to make the systems somewhat stable. But hardky work.\n",
    "sig_factor = 5e2          # 5e2\n",
    "partial_factor = 2e1      # 2e1\n",
    "\n",
    "# 1 initilize theta_k = (pi_k, mu_k, sigma_k)\n",
    "# pi_k = prior for class k (assume uniform distr.)\n",
    "Pi = np.ones(K)/K\n",
    "\n",
    "# mu_k = mean for class k (random subset of the data)\n",
    "# sigma_k = covariance for class k (variance of data)\n",
    "Mu = np.zeros([K, n_pix])\n",
    "Sigma2 = np.zeros([K, n_pix])\n",
    "epsilon = 1e-3\n",
    "for k in range(K):\n",
    "    idx = np.random.choice(N, int(N/K))\n",
    "    temp_data = X[idx]\n",
    "    Mu[k,:] = np.mean(temp_data, 0)\n",
    "    for p in range(n_pix):        \n",
    "        Sigma2[k, p] = sig_factor * (np.mean((temp_data[:, p] - Mu[k,p]) ** 2) + epsilon)\n",
    "\n",
    "\n",
    "# 2 iterate over E and M step\n",
    "itr = 1\n",
    "errs = 0\n",
    "while True:      # make some stop criterium\n",
    "    # E: \n",
    "    # 2.2 where P(x_i|thetaold_k) = prod_j(exp(-(x-muold_kj)^2 / 2*sigma_kj^2) / sqrt(2*Ã¥i*sigma_kj^2))\n",
    "    Post = np.ones([N, K])\n",
    "    m = 1e6\n",
    "    for n in range(N):   # To many loops but I am tired\n",
    "        for k in range(K):\n",
    "            for p in range(n_pix): #2e1\n",
    "                partial_post = partial_factor * np.exp(-(X[n, p]- Mu[k, p])**2 / (2 * Sigma2[k, p])) / np.sqrt(2 * np.pi * Sigma2[k, p])\n",
    "                if partial_post < m:\n",
    "                    m = partial_post\n",
    "                if partial_post == 0.0:\n",
    "                    errs += 1\n",
    "                    break\n",
    "                Post[n, k] *= partial_post\n",
    "            if errs:\n",
    "                break\n",
    "        if errs: \n",
    "            break\n",
    "    print('1', m)\n",
    "    print('#ERRORS: ', errs)\n",
    "    if errs:\n",
    "        break\n",
    "    #break\n",
    "    # 2.1 rnew_ik = piold_k * P(x_i|thetaold_k) / (sigmaold_k * piold_k * P(x_i|thetaold_k))\n",
    "    # rnew is p(class| pixel/image) \n",
    "    r = np.zeros([N, K])\n",
    "    m = 1e6\n",
    "    for n in range(N):\n",
    "        r_row = np.zeros(K)\n",
    "        for k in range(K):\n",
    "            r_row[k] = Post[n, k] * Pi[k]\n",
    "        if sum(r_row) < m:\n",
    "            m = sum(r_row)\n",
    "        r[n, :] = r_row / sum(r_row)\n",
    "\n",
    "    print('2', m)\n",
    "    # M: \n",
    "    # rnew_k = sum_i(rnew_ik), pinew_k = rnew_k / N\n",
    "    rk = sum(r, 0)\n",
    "    Pi = rk / N \n",
    "    print('min pi is: ', np.min(Pi))\n",
    "\n",
    "    # munew_k = sum_i(rnew_ik*xi)/rnew_k, \n",
    "    # sigmanew_k = sum_i(rnew_ik*<xi,xi>) / rnew_k -<munew_k,munew_k> (simplified and took variance)\n",
    "    Mu_new = np.zeros([K, n_pix])\n",
    "    Sigma2_new = np.zeros([K, n_pix])\n",
    "    for k in range(K):\n",
    "        for p in range(n_pix):\n",
    "            Mu_new[k, p] = np.dot(r[:,k], X[:,p]) / rk[k]\n",
    "            Sigma2_new[k, p] = sig_factor * (np.dot(r[:, k], (X[:, p] - Mu[k, p]) ** 2) / rk[k] + epsilon)\n",
    "\n",
    "    print('steps at iteration ', itr, ': ', la.norm(Mu_new - Mu), la.norm((Sigma2_new - Sigma2)/sig_factor), '\\n')\n",
    "    Mu = Mu_new\n",
    "    Sigma2 = Sigma2_new\n",
    "    # 3 stop when mu and sigma has converged\n",
    "    itr += 1\n",
    "    if itr > 5:\n",
    "        break\n",
    "Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55814516 0.47337239 0.81463811]\n",
      " [0.44185484 0.52662761 0.18536189]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.8145905601913872"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just me testing stuff\n",
    "\n",
    "mat = np.random.rand(2, 3)\n",
    "for i in range(3):\n",
    "    mat[:,i] /= sum(mat[:,i])\n",
    "i,j=np.shape(mat)\n",
    "v = np.ones(2)\n",
    "print(mat)\n",
    "r = np.random.choice(10, 3)\n",
    "v = np.array([1,2,3,4,5,6,7,8,9, 10])\n",
    "v[r]\n",
    "np.sum(mat,1)\n",
    "np.max(data_1_train)\n",
    "X = data_1_train\n",
    "M = np.mean(X)\n",
    "S = np.std(X)\n",
    "X = (X - M) / S\n",
    "np.min(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the result of the EM-algorithm (the found distribution parameters) to cluster the training data (essentially, using the resulting classifier to do a prediction over the training data). Produce a confusion matrix over the known labels for the training data and your EM-generated clusters. What do you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If necessary, find a way to \"repair\" the cluster assignments so that you can do a prediction run over the test data, from which you can compare the results with your earlier implementation of the Gaussian NBC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use now also the k-Means implementation from SciKitLearn and compare the results to yours (they should be similar at least in the sense that there are classes that are more clearly separated from the rest than others for both approaches). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your reporting task\n",
    "Write a brief (1-2 pages) report that discusses the following issues / answers the following questions. Give proper references in case you consult any material other than your implementation. \n",
    "\n",
    "1. In lab 5 you were asked to implement the \"statistics based\" NBC using counts over the encountered values for the attributes in each class. Why was that an oversimplification and how does the resulting problem relate to your first implementation of the decision tree in lab 2 for the digits data? \n",
    "\n",
    "2. The issue above was handled by switching to Normal distributions instead of the count based approach. Mitchell (see lecture slides lecture 10) suggests the m-estimate to solve the problem. What does the m-estimate do?\n",
    "\n",
    "3. In lab 5 you implemented a Nearest-Centroid-Classifier (NCC) and a Gaussian NBC. How are those related?\n",
    "\n",
    "4. Further, what is the relationship between the NCC and the result of the k-Means?\n",
    "\n",
    "5. Explain in your own words the difference between k-Means clustering and the basic EM for GMMs as given in the Murphy-book (see above).\n",
    "\n",
    "Submit the report in PDF-format (nothing else will be considered in this case) according to the instructions. OBS, this is Assignment 3 (report lab 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
