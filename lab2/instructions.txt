Lab session / assignment 2
*** Please note that there might be minor adaptations in the later part of the assignment during the coming days, but Part 1 is quite definite. Note also, that humans do produce errors, if you find any, let us know. ***

 

Objective

In this lab session you will explore some programming concepts in Python,
SciKitLearn, and partially Numpy to implement a decision tree classifier and
compare it with the provided one in SciKitLearn,
get acquainted with the simplified version of the MNIST dataset provided in
SciKitLearn, and explore the effect of (some) data preprocessing on
the learning process.
A classic algorithm for training / constructing decision trees is ID3.
However, in SciKitLearn, only one decision tree classifier is provided,
which is based on another algorithm, CART. ID3 has the advantage of being able
to handle multivalued attributes in the decisions, i.e., it is possible to
split the tree into arbitrarily many subtrees in any node, not only two.
 While it is of course still possible to use a binary tree structure also to
 represent rather complex value landscapes by making the decision as
 "one against all", the tree would obviously have to grow deeper,
 as the remaining decisions on the same attribute might still have to be made.
 To compare the built-in tree with an ID3-implementation,
 you will implement your own version of an ID3-decision tree classifier.
 For this, feel free to consult other implementations, one is for example
 available here, but you should refer to your source when presenting a
 solution as your own when it contains code provided by others.
 A zip-file with some code skeletons, useful snippets and a visualised tree for
 comparison is provided HERE.

To pass the assignment, you need to show an implementation that runs, produces sensible results and that you can explain individually when asked to do so. Also, you should have answers to the questions posed in the different sub tasks.

Your task consists of the following two blocks

1. Use and experiment with the built in SciKitLearn DecisionTreeClassifier (based on CART)
	1. Use the code skeleton / snippets provided or the notebook used in the tutorial
	from the first course week to load the digits dataset from the datasets provided
	in SciKitLearn. Inspect the data. What is in there?
	2. Split your data set into 70% training data (features and labels), and 30% test 	data.
	3. Set up a DecisionTreeClassifier as it comes in SciKitLearn. Use it with default 	parameters to train a decision tree classifier for the digits dataset based on the 	training data. Follow the tutorial (or the respective documentation) and produce a 	plot of the tree with graphviz. What can you learn from this about how the used 	algorithm handles the data?
	4. Test the classifier with the remaining test data and analyse it using the 		metrics packages of SciKitLearn (classification report, confusion matrix). What do 	you see?
	5. Change the parameters of the classifier, e.g., the minimum number of samples in 	a leaf / for a split, to see how the tree and the results are affected.

2. Implement your own decision tree classifier based on the ID3 algorithm and 		compare the results.
	1. Make a decision regarding the data structure that your tree should be able to 	handle. In the code handout (see above), you will find the tree assumed to be 		implemented with nodes that are dictionaries. 
	2. Inspect other parts of the code provided. You will find one example for how it 	is easily possible to construct the visualisation data (dot-data) for the 		graphviz-visualisation in parallel to the actual decision tree. Whenever a node is 	added to the tree, it can also immediately be added to the graph. Feel free to use 	this for your own implementation. 
	3. Simply running main in the handout will produce a tree with one node, 		visualised in testTree.pdf. Make sure that this works, i.e., that you have all the 	necessary libraries installed.
	4. The code handout contains a mere skeleton for the ID3 classifier. Implement 		what is needed to actually construct a decision tree classifier. Implement the ID3 	algorithm, e.g., according to what is provided in the lecture or on this page 		below. Use information gain as criterion for the best split attribute.
	5. Test your classifier with the toy example provided in the ToyData class given 	in the skeleton. In main you can also see how to make use of the dot-data to 		produce a visualisation with graphviz. The tree rendered in the file given to the 	respective method should look like the one given in toyTree.pdf. 
	6. When you are sure that everything works properly, run the ID3-training for the 	digits training data you used in part 1. Do not constrain the training, i.e., run 	with default parameters. What do you see in the plot? Analyse the result (produce 	a confusion matrix and a classification report) and compare with the result from 	part 1 (when running with default parameters).
	7. One striking difference should be in the ratio of breadth and depth of the two 	trees. Why is that the case? Modify your data set to contain only three values for 	the attributes (instead of potentially 16), e.g., 'dark', 'grey', and 'light', 		with for example 'dark' representing pixel values <5.0, and 'light' those >10.0. 	8. Train and test the classifier again. Do your results improve? Can you match the 	SKLearn implementation's accuracy? If not, why do you think this is the case?

(Bonus: If interested, explore the effects of different parameters regulating the depth of the tree, the maximum number of samples per leaf or required for a split, initially on the SKLearn version, but of course you can also implement them for your own classifier.)
 

The ID3 algorithm in pseudocode
ID3 builds a tree structure recursively. If you are unfamiliar with trees or recursion, it is strongly recommended to consult material from EDAA01 (Programming techniques, advanced course).

ID3 (Samples, Target_Attribute, Attributes)
  Create a (root) node Root for the tree

  If all samples belong to one class <class_name>
      Return the single-node tree Root, with label = <class_name>.
 
  If Attributes is empty, then
      Return the single node tree Root, with label = most common class value in Samples.
  else
      Begin
          Let A be the attribute a in Attributes that generates the maximum information gain
                when the tree is split based on a.

          Set A as the target_attribute of Root

          For each possible value, v, of A, add a new tree branch below Root,
               corresponding to the test A == v, i.e.,
              Let Samples(v) be the subset of samples that have the value v for A.
              If Samples(v) is empty, then
                  Below this new branch add a leaf node with label
                        = most common class value in Samples.
              else
                  Below this new branch add the subtree ID3 (Samples(vi), A, Attributes/{A})
        End
  Return Root